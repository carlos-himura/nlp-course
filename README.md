ðŸ§  Natural Language Processing in Python

This repository contains code, notebooks, and resources for a comprehensive Natural Language Processing (NLP) curriculum, progressing from traditional machine learning approaches to modern Deep Learning and Transformer-based architectures.

The goal is to build both theoretical understanding and practical intuition, starting with interpretable models and moving toward state-of-the-art NLP systems.

â¸»

Traditional NLP

Focused on interpretability, efficiency, and strong performance on small-to-medium-sized datasets using classical machine learning techniques.

Topics covered:
	â€¢	Sentiment Analysis
Rule-based approaches using VADER
	â€¢	Text Classification
Supervised learning with NaÃ¯ve Bayes and Logistic Regression
	â€¢	Topic Modeling
Unsupervised learning using Non-Negative Matrix Factorization (NMF)

â¸»

Modern NLP (Transformers & LLMs)

Focused on high performance, contextual understanding, and complex language tasks using the Hugging Face Transformers ecosystem.

Topics covered:
	â€¢	Encoder-Only Models
	â€¢	BERT for Sentiment Analysis and Named Entity Recognition (NER)
	â€¢	Encoderâ€“Decoder Models
	â€¢	BART and T5 for Text Summarization and Zero-Shot Classification
	â€¢	Decoder-Only Models
	â€¢	GPT-style models for Text Generation
	â€¢	Text Embeddings & Similarity
	â€¢	MiniLM embeddings with Cosine Similarity for:
	â€¢	Document similarity
	â€¢	Semantic search
	â€¢	Recommendation systems
